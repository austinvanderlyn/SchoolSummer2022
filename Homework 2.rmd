---
title: "Homework 2"
author: "Austin Vanderlyn ajl745"
date: "7/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 6.1

#### 6.1.a

##### Start R and use these commands to load the data;

```{r}
library(caret)
data(tecator)
```

##### the matrix absorp contains the 100 absorbance values for the 215 samples, while matrix endpoints contains the percent of moisture, fat, and protein in columns 1-3, respectively


#### 6.1.b

##### Use PCA to determine the effective dimension of these data. What is the effective dimension?

Run PCA on absorb;
```{r}
PCA.ab = prcomp(absorp,
                center = TRUE,
                scale = TRUE)
```


To determine the principal components, calculate variance explained by each component;
```{r}
var = PCA.ab$sdev^2/sum(PCA.ab$sdev^2)*100
head(var)
```

This shows that almost all of the variance is explained by the first component, which is much less than the number of predictors.


#### 6.1.c

##### Split the data into a training and a test set for the response of the percentage of moisture, pre-process the data, and build each variety of models described in this chapter. For those models with tuning parameters, what are the optimal values of the tuning parameters?

Split data;
```{r}
set.seed(123)
ab = data.frame(absorp)
split = createDataPartition(endpoints[, 3],
                            p = 0.8,
                            list = FALSE)

absorpTrain = ab[split,]
absorpTest = ab[-split,]
endTrain = endpoints[split, 3]
endTest = endpoints[-split,3]

```

Train control;
```{r}
ctrl = trainControl(method = "repeatedcv",
                    repeats = 10)
```

Linear Model;
```{r}
set.seed(123)
ab.lm = train(absorpTrain, endTrain,
              method = "lm",
              trControl = ctrl)
ab.lm
```

PLS model;
```{r}
set.seed(123)
pls.lm = train(absorpTrain, endTrain,
              method = "pls",
              tuneGrid = expand.grid(ncomp = 1:50),
              trControl = ctrl)
pls.lm
```

Plot PLS model;
```{r}
plot(pls.lm)
```

PCR model;
```{r}
set.seed(123)
pcr.lm = train(absorpTrain, endTrain,
              method = "pcr",
              preProcess = c("center", "scale"),
              tuneGrid = expand.grid(ncomp = 1:50),
              trControl = ctrl)
pcr.lm
```

```{r}
plot(pcr.lm)
```

Ridge Regression model;
```{r}
set.seed(123)
ptm = proc.time()
library(elasticnet)

ridgeGrid = expand.grid(lambda = seq(0, .1, 
                                     length = 10))

ridge.lm = train(absorpTrain, endTrain,
                 method = "ridge",
                 tuneGrid = ridgeGrid,
                 trControl = ctrl,
                 preProcess = c("center", "scale"))

ridge.lm
```

ENET Model;
```{r}
set.seed(123)

enetGrid = expand.grid(lambda = c(0, 0.01, .1),
                       fraction = seq(.05, 1,
                                      length = 20))

enet.lm = train(absorpTrain, endTrain,
                method = "enet",
                tuneGrid = enetGrid,
                trControl = ctrl,
                preProcess = c("center", "scale"))

enet.lm

```


```{r}
plot(enet.lm)
```


```{r}
plot(ridge.lm)
```


#### 6.1.d

##### Which model has the best predictive ability? Is any model significantly better or worse than the others?


Store predictions;
```{r}
testResults = data.frame(obs = absorpTest,
                         Linear_Regression = predict(ab.lm, absorpTest))

testResults$LRM = predict(pcr.lm, absorpTest)

testResults$PLS = predict(pls.lm, absorpTest)

testResults$PCR = predict(pcr.lm, absorpTest)

testResults$ENET = predict(enet.lm, absorpTest)

testResults$Ridge = predict(ridge.lm, absorpTest)

```


Create data frame of scores;
```{r}
R2 = RMSE = MAE = numeric(0)

R2[1] = cor(testResults$LRM, endTest)^2
RMSE[1] = sqrt(mean((testResults$LRM - endTest)^2))
MAE[1] = mean(abs(testResults$LRM - endTest))

R2[2] = cor(testResults$PCR, endTest)^2
RMSE[2] = sqrt(mean((testResults$PCR - endTest)^2))
MAE[2] = mean(abs(testResults$PCR - endTest))

R2[3] = cor(testResults$PLS, endTest)^2
RMSE[3] = sqrt(mean((testResults$PLS - endTest)^2))
MAE[3] = mean(abs(testResults$PLS - endTest))

R2[4] = cor(testResults$Ridge, endTest)^2
RMSE[4] = sqrt(mean((testResults$Ridge - endTest)^2))
MAE[4] = mean(abs(testResults$Ridge - endTest))

R2[5] = cor(testResults$ENET, endTest)^2
RMSE[5] = sqrt(mean((testResults$ENET - endTest)^2))
MAE[5] = mean(abs(testResults$ENET - endTest))

results = cbind(R2, RMSE, MAE)
row.names(results) = c("LRM", "PCR", "PLS", "Ridge", "ENET")
results
```


The models all have relatively similar R2, with the exception of the ridge regression, which has a much smaller R2, 74%. The ridge regression also has the highest RMSE and a pretty high MAE. The only other big outlier is the PLS model, which has a similar R2 but a crazy high MAE. Overall, I'd say none really stand out as being significantly better than the others, but a couple stand out as worse than the others; namely, the ridge regression and PLS. 


#### 6.1.e

##### Explain which model you would use for predicting the percentage of moisture of a sample.

If I had to pick one, I would select the ENET model. It has the highest R2, the lowest RMSE and the lowest MAE. Overall, it seems very stable and accurate for predictions.


### Exercise 6.2

##### Developing a model to predict permeability (see Sect. 1.4) could save significant resources for a pharmaceutical company, while at the same time more rapidly identifying molecules that have a sufficient permeability to become a drug:

#### 6.2.a

##### Start R and use these commands to load the data:

##### The matrix fingerprints contains the 1,107 binary molecular predictors for the 165 compounds, while permeability contains permeability response.

```{r}
library(AppliedPredictiveModeling)
data("permeability")
```


#### 6.2.b

##### The fingerprint predictors indicate the presence or absence of substructures of a molecule and are often sparse meaning that relatively few of the molecules contain each substructure. Filter out the predictors that have low frequencies using the nearZeroVar function from the caret package. How many predictors are left for modeling?

```{r}
zero.var = nearZeroVar(fingerprints)
fingerprints = data.frame(fingerprints)
fingerprints = fingerprints[-zero.var]
```

There were originally 1107 predictors, and after filtering out the ones with near zero variance, there are now only 388 left for modeling.


#### 6.2.c

##### Split the data into a training and a test set, pre-process the data, and tune a PLS model. How many latent variables are optimal and what is the corresponding resampled estimate of R2?

View permeability distribution to check for normality;
```{r}
hist(permeability)
```

This shows the frequency for values of permeability is very right skewed, so we should do some form of transformation to make it more normal;
```{r}
library(MASS)
par(mfrow = c(2,2))
pp1 = scale(permeability, scale = FALSE)
hist(pp1)
pp2 = scale(permeability,
            center = TRUE,
            scale = FALSE)
hist(pp2)
hist(log(permeability))
hist(log10(permeability))
```

a regular log transformation does the best job transforming the distribution to something more normal, so I'll apply that transformation to the permeability variable;
```{r}
permeability = log(permeability)
```

Test for collinearity;
```{r}
segCorr = cor(fingerprints)
highCorr = findCorrelation(segCorr, .90)
fingerprints2 = fingerprints[-highCorr]
```



Split data into training and testing;
```{r}
set.seed(123)
perm.split = createDataPartition(permeability,
                                 p = 0.75,
                                 list = FALSE)

train.finger = fingerprints[perm.split,]
train.perm = permeability[perm.split,]

test.finger = fingerprints[-perm.split,]
test.perm = permeability[-perm.split,]
```


```{r}
set.seed(123)
ctrl2 = trainControl(method = "repeatedcv") 

plsTune = train(train.finger, train.perm,
                method = "pls",
                tuneGrid = expand.grid(ncomp = 1:15),
                trControl = ctrl2)
plsTune
```













