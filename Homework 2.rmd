---
title: "Homework 2"
author: "Austin Vanderlyn ajl745"
date: "7/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 6.1

#### 6.1.a

##### Start R and use these commands to load the data;

```{r}
library(caret)
data(tecator)
```

##### the matrix absorp contains the 100 absorbance values for the 215 samples, while matrix endpoints contains the percent of moisture, fat, and protein in columns 1-3, respectively


#### 6.1.b

##### Use PCA to determine the effective dimension of these data. What is the effective dimension?

Run PCA on absorb;
```{r}
PCA.ab = prcomp(absorp,
                center = TRUE,
                scale = TRUE)
```


To determine the principal components, calculate variance explained by each component;
```{r}
var = PCA.ab$sdev^2/sum(PCA.ab$sdev^2)*100
head(var)
```

This shows that almost all of the variance is explained by the first component, which is much less than the number of predictors.


#### 6.1.c

##### Split the data into a training and a test set for the response of the percentage of moisture, pre-process the data, and build each variety of models described in this chapter. For those models with tuning parameters, what are the optimal values of the tuning parameters?

Split data;
```{r}
set.seed(123)
ab = data.frame(absorp)
split = createDataPartition(endpoints[, 3],
                            p = 0.8,
                            list = FALSE)

absorpTrain = ab[split,]
absorpTest = ab[-split,]
endTrain = endpoints[split, 3]
endTest = endpoints[-split,3]

```

Train control;
```{r}
ctrl = trainControl(method = "repeatedcv",
                    repeats = 10)
```

Linear Model;
```{r}
set.seed(123)
ab.lm = train(absorpTrain, endTrain,
              method = "lm",
              trControl = ctrl)
ab.lm
```

PLS model;
```{r}
set.seed(123)
pls.lm = train(absorpTrain, endTrain,
              method = "pls",
              tuneGrid = expand.grid(ncomp = 1:50),
              trControl = ctrl)
pls.lm
```

Plot PLS model;
```{r}
plot(pls.lm)
```

PCR model;
```{r}
set.seed(123)
pcr.lm = train(absorpTrain, endTrain,
              method = "pcr",
              preProcess = c("center", "scale"),
              tuneGrid = expand.grid(ncomp = 1:50),
              trControl = ctrl)
pcr.lm
```

```{r}
plot(pcr.lm)
```

Ridge Regression model;
```{r}
set.seed(123)
ptm = proc.time()
library(elasticnet)

ridgeGrid = expand.grid(lambda = seq(0, .1, 
                                     length = 10))

ridge.lm = train(absorpTrain, endTrain,
                 method = "ridge",
                 tuneGrid = ridgeGrid,
                 trControl = ctrl,
                 preProcess = c("center", "scale"))

ridge.lm
```

ENET Model;
```{r}
set.seed(123)

enetGrid = expand.grid(lambda = c(0, 0.01, .1),
                       fraction = seq(.05, 1,
                                      length = 20))

enet.lm = train(absorpTrain, endTrain,
                method = "enet",
                tuneGrid = enetGrid,
                trControl = ctrl,
                preProcess = c("center", "scale"))

enet.lm

```


```{r}
plot(enet.lm)
```


```{r}
plot(ridge.lm)
```


#### 6.1.d

##### Which model has the best predictive ability? Is any model significantly better or worse than the others?


Store predictions;
```{r}
testResults = data.frame(obs = absorpTest,
                         Linear_Regression = predict(ab.lm, absorpTest))

testResults$LRM = predict(pcr.lm, absorpTest)

testResults$PLS = predict(pls.lm, absorpTest)

testResults$PCR = predict(pcr.lm, absorpTest)

testResults$ENET = predict(enet.lm, absorpTest)

testResults$Ridge = predict(ridge.lm, absorpTest)

```


Create data frame of scores;
```{r}
R2 = RMSE = MAE = numeric(0)

R2[1] = cor(testResults$LRM, endTest)^2
RMSE[1] = sqrt(mean((testResults$LRM - endTest)^2))
MAE[1] = mean(abs(testResults$LRM - endTest))

R2[2] = cor(testResults$PCR, endTest)^2
RMSE[2] = sqrt(mean((testResults$PCR - endTest)^2))
MAE[2] = mean(abs(testResults$PCR - endTest))

R2[3] = cor(testResults$PLS, endTest)^2
RMSE[3] = sqrt(mean((testResults$PLS - endTest)^2))
MAE[3] = mean(abs(testResults$PLS - endTest))

R2[4] = cor(testResults$Ridge, endTest)^2
RMSE[4] = sqrt(mean((testResults$Ridge - endTest)^2))
MAE[4] = mean(abs(testResults$Ridge - endTest))

R2[5] = cor(testResults$ENET, endTest)^2
RMSE[5] = sqrt(mean((testResults$ENET - endTest)^2))
MAE[5] = mean(abs(testResults$ENET - endTest))

results = cbind(R2, RMSE, MAE)
row.names(results) = c("LRM", "PCR", "PLS", "Ridge", "ENET")
results
```


The models all have relatively similar R2, with the exception of the ridge regression, which has a much smaller R2, 74%. The ridge regression also has the highest RMSE and a pretty high MAE. The only other big outlier is the PLS model, which has a similar R2 but a crazy high MAE. Overall, I'd say none really stand out as being significantly better than the others, but a couple stand out as worse than the others; namely, the ridge regression and PLS. 


#### 6.1.e

##### Explain which model you would use for predicting the percentage of moisture of a sample.

If I had to pick one, I would select the ENET model. It has the highest R2, the lowest RMSE and the lowest MAE. Overall, it seems very stable and accurate for predictions.


### Exercise 6.2

##### Developing a model to predict permeability (see Sect. 1.4) could save significant resources for a pharmaceutical company, while at the same time more rapidly identifying molecules that have a sufficient permeability to become a drug:

#### 6.2.a

##### Start R and use these commands to load the data:

##### The matrix fingerprints contains the 1,107 binary molecular predictors for the 165 compounds, while permeability contains permeability response.

```{r}
library(AppliedPredictiveModeling)
data("permeability")
```


#### 6.2.b

##### The fingerprint predictors indicate the presence or absence of substructures of a molecule and are often sparse meaning that relatively few of the molecules contain each substructure. Filter out the predictors that have low frequencies using the nearZeroVar function from the caret package. How many predictors are left for modeling?

```{r}
zero.var = nearZeroVar(fingerprints)
fingerprints = data.frame(fingerprints)
fingerprints = fingerprints[-zero.var]
```

There were originally 1107 predictors, and after filtering out the ones with near zero variance, there are now only 388 left for modeling.


#### 6.2.c

##### Split the data into a training and a test set, pre-process the data, and tune a PLS model. How many latent variables are optimal and what is the corresponding resampled estimate of R2?

View permeability distribution to check for normality;
```{r}
hist(permeability)
```

This shows the frequency for values of permeability is very right skewed, so we should do some form of transformation to make it more normal;
```{r}
library(MASS)
par(mfrow = c(2,2))
pp1 = scale(permeability, scale = FALSE)
hist(pp1)
pp2 = scale(permeability,
            center = TRUE,
            scale = FALSE)
hist(pp2)
hist(log(permeability))
hist(log10(permeability))
```

a regular log transformation does the best job transforming the distribution to something more normal, so I'll apply that transformation to the permeability variable;
```{r}
permeability = log(permeability)
```

Test for collinearity;
```{r}
segCorr = cor(fingerprints)
highCorr = findCorrelation(segCorr, .90)
fingerprints2 = fingerprints[-highCorr]
```



Split data into training and testing;
```{r}
set.seed(123)
perm.split = createDataPartition(permeability,
                                 p = 0.75,
                                 list = FALSE)

train.finger = fingerprints[perm.split,]
train.perm = permeability[perm.split,]

test.finger = fingerprints[-perm.split,]
test.perm = permeability[-perm.split,]
```


```{r}
set.seed(123)
ctrl2 = trainControl(method = "repeatedcv") 

plsTune = train(train.finger, train.perm,
                method = "pls",
                tuneGrid = expand.grid(ncomp = 1:50),
                trControl = ctrl2)
plsTune
```


plot the initial pls model with R^2;
```{r}
plot(plsTune,
     metric = "Rsquared")
```


The optimal number of components to use is 8 and the corresponding R^2 is 0.5929008.

```{r}
set.seed(123)
plsTune = train(train.finger, train.perm,
                method = "pls",
                tuneGrid = expand.grid(ncomp = 8),
                trControl = ctrl2)
plsTune
```




#### 6.2.d

##### Predict the response for the test set. What is the test set estimate of R2?

Predicted response for thes test set;
```{r}
pls.predict = predict(plsTune, test.finger,
                      ncomp = 8)
xyplot(test.perm ~ pls.predict,
       xlab = "Observed",
       ylab = "Predicted",
       type = c("p", "r"),
       xlim = c(-1:5),
       ylim = c(-1:5))
```

```{r}
rss.pls = sum((pls.predict - test.perm)^2)
tss.pls = sum((test.perm - mean(test.perm))^2)
r2.pls = 1 - rss.pls/tss.pls
r2.pls
```

The test set estimate of R2 is 0.31, not that great.


#### 6.2.e

##### Try building other models discussed in this chapter. Do any have better predictive performance?

The three main other types of models discussed in this chapter are the PCR, ridge regression, and elastic net model, so I'll fit one of each of those.

PCR model;
```{r}
set.seed(123)
pcrTune <- train(train.finger, train.perm,
                 method = "pcr",
                 metric = "Rsquared",
                 tuneGrid = expand.grid(ncomp = 1:50),
                 trControl = ctrl)
pcrTune                  
plot(pcrTune)
```


Ridge Regression Model;
```{r}
set.seed(123)
ptm = proc.time()
library(elasticnet)

ridgeGrid = expand.grid(lambda = seq(0, .1, 
                                     length = 10))

ridgeTune = train(train.finger, train.perm,
                 method = "ridge",
                 metric = "Rsquared",
                 tuneGrid = ridgeGrid,
                 trControl = ctrl)

ridgeTune
```

```{r}
plot(ridgeTune)
```


ENET model;
```{r}
set.seed(123)

enetGrid = expand.grid(lambda = c(0, 0.01, .1),
                       fraction = seq(.05, 1,
                                      length = 20))

enetTune = train(train.finger, train.perm,
                method = "enet",
                metric = "Rsquared",
                tuneGrid = enetGrid,
                trControl = ctrl)

enetTune
```

```{r}
plot(enetTune)
```


Store predictions;
```{r}
test2Results = data.frame(obs = test.finger,
                         PLS = predict(plsTune, test.finger))

test2Results$PCR = predict(pcrTune, test.finger)

test2Results$ENET = predict(enetTune, test.finger)

test2Results$Ridge = predict(ridgeTune, test.finger)
```


Create data frame of scores;
```{r}
r2 = rmse = mae = numeric(0)

r2[1] = cor(test2Results$PLS, test.perm)^2
rmse[1] = sqrt(mean((test2Results$PLS - test.perm)^2))
mae[1] = mean(abs(test2Results$PLS - test.perm))

r2[2] = cor(test2Results$PCR, test.perm)^2
rmse[2] = sqrt(mean((test2Results$PCR - test.perm)^2))
mae[2] = mean(abs(test2Results$PCR - test.perm))

r2[3] = cor(test2Results$Ridge, test.perm)^2
rmse[3] = sqrt(mean((test2Results$Ridge - test.perm)^2))
mae[3] = mean(abs(test2Results$Ridge - test.perm))

r2[4] = cor(test2Results$ENET, test.perm)^2
rmse[4] = sqrt(mean((test2Results$ENET - test.perm)^2))
mae[4] = mean(abs(test2Results$ENET - test.perm))

results2 = cbind(r2, rmse, mae)
row.names(results2) = c("PLS", "PCR", "Ridge", "ENET")
results2
```

It looks like the PLS performs the best of the models.


### Exercise 7.4

##### Return to the permeability problem outlined in Exercise 6.2. Train several nonlinear regression models and evaluate the resampling and test set performance.


#### 7.4.a

##### Return to the permeability problem outlined in Exercise 6.2. Train several nonlinear regression models and evaluate the resampling and test set performance.

The models discussed in chapter 7 are the Neural Net, MARS model, Support Vector Machines, and K-Nearest Neighbors, so I will build a model for each of those.

Neural Net;
```{r}
nnetGrid = expand.grid(decay = c(0, 0.01, .1),
                       size = c(1, 3, 5, 7),
                       bag = FALSE)
```


```{r}
set.seed(123)
nnetTune = train(absorpTrain, endTrain,
                 method = "avNNet",
                 tuneGrid = nnetGrid,
                 trControl = ctrl,
                 preProc = c("center", "scale"),
                 linout = TRUE,
                 trace = FALSE,
                 MaxNWts = 1000,
                 maxit = 1000,
                 allowParallel = FALSE)
```

```{r}
nnetTune
```

```{r}
plot(nnetTune)

test3Results = data.frame(obs = endTest,
                          NNET = predict(nnetTune, absorpTest))
```


MARS Model;
```{r}
set.seed(123)
marsTune = train(absorpTrain, endTrain,
                 method = "earth",
                 tuneGrid = expand.grid(degree = 1,
                                        nprune = 2:38),
                 trControl = ctrl)
marsTune
```

```{r}
plot(marsTune)
```

```{r}
marsImp = varImp(marsTune,
                 scale = FALSE)
plot(marsImp,
     top = 10)

test3Results$Mars = predict(marsTune, absorpTest)
```


SVM Model
```{r}
set.seed(123)
svmTune = train(absorpTrain, endTrain,
                method = "svmRadial",
                preProc = c("center", "scale"),
                tuneLength = 14,
                trControl = ctrl)
svmTune
```

```{r}
plot(svmTune)

test3Results$SVM = predict(svmTune, absorpTest)
```


SVM Polynomial model;
```{r}
svmGrid = expand.grid(degree = 1:2,
                      scale = c(0.01, 0.005, 0.001),
                      C = 2^(-2:5))
```


```{r}
set.seed(123)
svmPTune = train(absorpTrain, endTrain,
                method = "svmPoly",
                preProc = c("center", "scale"),
                tuneGrid = svmGrid,
                trControl = ctrl)
svmPTune
```

```{r}
plot(svmPTune)

test3Results$SVMp = predict(svmPTune, absorpTest)
```

```{r}
set.seed(123)
knnTune = train(absorpTrain, endTrain,
                method = "knn",
                tuneGrid = data.frame(k = 1:20),
                trControl = ctrl)
knnTune
```

```{r}
plot(knnTune)

test3Results$KNN = predict(knnTune, absorpTest)
```


```{r}
set.seed(123)
Nnet.pred = predict(nnetTune, absorpTest)
MARS.pred = predict(marsTune, absorpTest)
SVM.pred = predict(svmTune, absorpTest)
SVMp.pred = predict(svmPTune, absorpTest)
knn.pred = predict(knnTune, absorpTest[, names(absorpTrain)])

data.frame(rbind(NNET = postResample(pred = Nnet.pred, obs = endTest),
                 MARS = postResample(pred = MARS.pred, obs = endTest),
                 SVM = postResample(pred = SVM.pred, obs = endTest),
                 SVMp = postResample(pred = SVMp.pred, obs = endTest),
                 KNN = postResample(pred = knn.pred, obs = endTest)))
```

We get much better results with the Neural Net model, with a RMSE of only 0.42 and an R^2 close to 1. However, depending on what kind of data set is being worked on, that might be a bad choice. My computer took a RIDICULOUSLY long time to process the neural net, the polynomial SVM might be a better option, and its processing time was much more reasonable.










