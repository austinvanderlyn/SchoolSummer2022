---
title: "Exercise 5"
author: "Austin Vanderlyn ajl745"
date: "7/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 5

#### This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapter's lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.

Libraries;
```{r}
library(AppliedPredictiveModeling)
library(caret)
library(ISLR)
library(corrplot)
```

Data;
```{r}
data("Weekly")
```



### 5.a

#### Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns? 

Data exploration;
```{r}
head(Weekly)
```

```{r}
plot(Weekly$Volume,
     col = Weekly$Year)
```

Check correlation;
```{r}
wkcor = cor(Weekly[,1:8])
corrplot(wkcor)
```

The only variables that appear to have any significant correlation are Year and Volume, but this isn't really surprising since we're dealing with stock market prices. The stock values don't restart at 0 in a near year, they build off of where the stocks finished at the end of the previous year, so we would expect them to be correlated.


### 5.b

#### Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

Fit a logistic regression model;
```{r}
set.seed(123)
glmGrid = trainControl(method = "repeatedcv",
                       repeats = 5)

glmTune = train(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                data = Weekly,
                family = "binomial",
                method = "glm",
                trControl = glmGrid)

summary(glmTune)
```

The only predictor that appears to be significant at the default of alpha = 0.05 is Lag 2, with a p-value of 0.0296.


### 5.c

#### Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

Compute confusion matrix;
```{r}
set.seed(123)
preds = predict(glmTune, Weekly)

confusionMatrix(preds, Weekly$Direction)
```


Wow, not the greatest accuracy rate, only 56.11%.

As for the types of errors made, the model is extremely lopsided. Down and Up are a little different from a usual binary predictor 0/1 but if we take down to be 0 and Up to be 1, then the model is way worse when it comes to Type I errors; that is, it predicts Up and actually is Down, a false positive. 

There are 478 misclassifications, and 430 of them are Type I errors, 90% of the total errors. Type II (false negative) are only 10% of the errors.


### 5.d

#### Now fit the logistic regression model using a training data period from 1990 to 2008. Compute the confusion matrix and the overall fraction of correct predictions for the held out data.

create the training and testing set;
```{r}
train = (Weekly$Year <2009)
weeklyTrain = Weekly[train,]
weeklyTest = Weekly[!train,]
```


fit logit model using training data;
```{r}
set.seed(123)
glmGrid = trainControl(method = "repeatedcv",
                       repeats = 5)

glmTune2 = train(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                data = weeklyTrain,
                family = "binomial",
                method = "glm",
                trControl = glmGrid)

summary(glmTune2)
```


Compute confusion matrix;
```{r}
set.seed(123)
preds2 = predict(glmTune2, weeklyTest)

confusionMatrix(preds2, weeklyTest$Direction)
```


The accuracy of the model actually went down predicting on the test set, though the types of errors are a little bit more balanced out. The overall number of correct predictions on the held out data is 46.15%.


### 5.d

#### Repeat (d) using LDA

Fit LDA model;
```{r}
set.seed(476)

ctrl = trainControl(method = "LGOCV",
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE,
                    savePredictions = TRUE)

ldaTune = train(Direction ~ .,
                data = weeklyTrain,
                method = "lda",
                preProc = c("center", "scale"),
                metric = "ROC",
                trControl = ctrl)
ldaTune
```

Calculate confusion matrix;
```{r}
set.seed(123)
predslda = predict(ldaTune, weeklyTest)

confusionMatrix(predslda, weeklyTest$Direction)
```

The LDA model has vastly better performance, 93.27%, and no points in the test set misclassified as Down.


### 5.f

#### Repeat using Partial Least Squares Discriminant Analysis

Fit PLSDA model;
```{r}
set.seed(123)
plsdaTune = train(Direction ~ .,
                  data = weeklyTrain,
                  method = "pls",
                  metric = "ROC",
                  tuneGrid = expand.grid(.ncomp = 1:5),
                  trControl = ctrl)
plot(plsdaTune)
```

Compute confusion matrix;
```{r}
set.seed(123)
predsplsda = predict(plsdaTune, weeklyTest)

confusionMatrix(predsplsda, weeklyTest$Direction)
```


Even better performance with the PLSDA model; 98.08% accuracy on the test set predictions.


### 5.g

#### Repeat (d) using Nearest Shrunken Centroids

Fit NSC model;
```{r}
nscGrid = data.frame(.threshold = 0:25)
nscTune = train(Direction ~ .,
                data = weeklyTrain,
                method = "pam",
                preProc = c("center", "scale"),
                tuneGrid = nscGrid,
                metric = "ROC",
                trControl = ctrl)
nscTune
plot(nscTune)
```

Compute confusion matrix;
```{r}
set.seed(123)
predsnsc = predict(nscTune, weeklyTest)

confusionMatrix(predsnsc, weeklyTest$Direction)
```

There's a big step down for the nearest shrunken centroids model, down to 83.65% accuracy on the test set. There's also a lopsided error effect like in the logistic model; all of the errors were misclassifications of Down, whereas Up was always correctly predicted.


### 5.h

#### Which of these methods appears to provide the best results on this data?

Make table;
```{r}
set.seed(123)
testResults = data.frame(rbind(Logistic = postResample(preds2, weeklyTest$Direction),
                         LDA = postResample(predslda, weeklyTest$Direction),
                         PLSDA = postResample(predsplsda, weeklyTest$Direction),
                         NSC = postResample(predsnsc, weeklyTest$Direction)))
testResults
```

The PLSDA model performs best, with an accuracy of 98.07%. I excluded the first logistic model because it was trained on the entire dataset instead of a specific time period like the others, so can't really compare results to them. The LDA performed well too, but PLSDA performs just a little bit better. 































